---
title: "R Survey Analysis"
output: html_document
---

## Setting up your R workspace and downloading necessary packages // libraries 

```{r setup}
rm(list=ls())
getwd()
setwd("/Users/allieblaising/desktop")

## Install packages outside of notebook environment. 

install.packages(c("psych" ,"xtable", "GPArotation", "tidyverse", "purrr", "dplyr", "jsonlite", "likert", "lavaan", "dplyr", "stringr", "ggplot2", 
                   "caret", "nFactors", "GPArotation", "Scale"))

library(psych)
library(xtable)
library(GPArotation)
library(psych)
library(dplyr)
library(jsonlite)
library(likert) 
library(lavaan) 
library(stringr)
library(ggplot2)
library(caret)
library(nFactors)
library(GPArotation)
library(Scale)
```

## Inputting main Json with midsurvey: 

```{r}
input = '[{"userID":"qL44K5kHvqZwHuzcAAAA","room":"B","name":"niceWolf","round":1,"midSurvey":["1=Strongly Disagree","2=Neutral","3=Agree","4=Neutral","5=Neutral","6=Agree","7=Agree","8=Agree","9=Agree","10=Agree","11=Agree","12=Agree","13=Agree","14=Agree","15=Yes"],"batch":1532028198071,"_id":"7CSvj5od8c38l7ZF"}
,{"userID":"1pob6EHCHwzLa7nUAAAB","room":"A","name":"culturedCat","round":1,"midSurvey":["1=Agree","2=Agree","3=Strongly Agree","4=Strongly Agree","5=Agree","6=Strongly Agree","7=Strongly Agree","8=Agree","9=Agree","10=Agree","11=Strongly Agree","12=Agree","13=Strongly Agree","14=Agree","15=Yes"],"batch":1532028198071,"_id":"rswvLzmgBBSPR2sz"}
,{"userID":"FGzkFbpp8ejWVSm0AAAC","room":"B","name":"niceRabbit","round":1,"midSurvey":["1=Neutral","2=Neutral","3=Neutral","4=Neutral","5=Neutral","6=Neutral","7=Neutral","8=Disagree","9=Disagree","10=Disagree","11=Strongly Disagree","12=Strongly Disagree","13=Strongly Disagree","14=Disagree","15=No"],"batch":1532028198071,"_id":"8E893jSKIj97pYVV"}
,{"userID":"-2emRdtAMYWaULymAAAD","room":"A","name":"conventionalDeer","round":1,"midSurvey":["1=Disagree","2=Disagree","3=Disagree","4=Disagree","5=Disagree","6=Neutral","7=Neutral","8=Disagree","9=Neutral","10=Neutral","11=Neutral","12=Disagree","13=Neutral","14=Disagree","15=No"],"batch":1532028198071,"_id":"u7JiHQ0GbMtl1K1f"}
,{"userID":"qL44K5kHvqZwHuzcAAAA","room":"A","name":"niceWolf","round":2,"midSurvey":["1=Disagree","2=Disagree","3=Disagree","4=Neutral","5=Neutral","6=Neutral","7=Disagree","8=Neutral","9=Neutral","10=Neutral","11=Agree","12=Agree","13=Neutral","14=Agree","15=Yes"],"batch":1532028198071,"_id":"PGPqlkL9yVpMiMYL"}
,{"userID":"1pob6EHCHwzLa7nUAAAB","room":"B","name":"culturedCat","round":2,"midSurvey":["1=Disagree","2=Strongly Disagree","3=Strongly Disagree","4=Disagree","5=Strongly Disagree","6=Disagree","7=Strongly Disagree","8=Disagree","9=Disagree","10=Strongly Disagree","11=Disagree","12=Disagree","13=Strongly Disagree","14=Disagree","15=No"],"batch":1532028198071,"_id":"E9VRKdzucacS0yyZ"}
,{"userID":"FGzkFbpp8ejWVSm0AAAC","room":"B","name":"niceRabbit","round":2,"midSurvey":["1=Disagree","2=Strongly Disagree","3=Strongly Disagree","4=Disagree","5=Strongly Disagree","6=Disagree","7=Neutral","8=Strongly Disagree","9=Neutral","10=Strongly Disagree","11=Neutral","12=Strongly Disagree","13=Disagree","14=Disagree","15=No"],"batch":1532028198071,"_id":"p0QnSqMlmFSbVHLF"}
,{"userID":"-2emRdtAMYWaULymAAAD","room":"A","name":"conventionalDeer","round":2,"midSurvey":["1=Disagree","2=Neutral","3=Disagree","4=Strongly Disagree","5=Neutral","6=Neutral","7=Neutral","8=Disagree","9=Disagree","10=Disagree","11=Neutral","12=Strongly Disagree","13=Neutral","14=Disagree","15=No"],"batch":1532028198071,"_id":"RYtBtN1O5HbwVUgX"}
,{"userID":"-2emRdtAMYWaULymAAAD","room":"A","name":"conventionalDeer","round":3,"midSurvey":["1=Agree","2=Agree","3=Agree","4=Agree","5=Strongly Agree","6=Strongly Agree","7=Strongly Agree","8=Strongly Agree","9=Agree","10=Strongly Agree","11=Agree","12=Strongly Agree","13=Agree","14=Strongly Agree","15=Yes"],"batch":1532028198071,"_id":"BBXovwXR3zgzuoXe"}
,{"userID":"FGzkFbpp8ejWVSm0AAAC","room":"B","name":"niceRabbit","round":3,"midSurvey":["1=Strongly Agree","2=Agree","3=Strongly Agree","4=Agree","5=Agree","6=Agree","7=Strongly Agree","8=Strongly Agree","9=Agree","10=Strongly Agree","11=Strongly Agree","12=Agree","13=Strongly Agree","14=Strongly Agree","15=Yes"],"batch":1532028198071,"_id":"7aR6XVuPSJsPmrkW"}
,{"userID":"1pob6EHCHwzLa7nUAAAB","room":"A","name":"culturedCat","round":3,"midSurvey":["1=Agree","2=Strongly Agree","3=Strongly Agree","4=Agree","5=Agree","6=Agree","7=Agree","8=Agree","9=Neutral","10=Agree","11=Strongly Agree","12=Agree","13=Agree","14=Strongly Agree","15=Yes"],"batch":1532028198071,"_id":"JCqaTTQhS7bpNmna"}
,{"userID":"qL44K5kHvqZwHuzcAAAA","room":"B","name":"niceWolf","round":3,"midSurvey":["1=Agree","2=Strongly Agree","3=Strongly Agree","4=Agree","5=Strongly Agree","6=Agree","7=Strongly Agree","8=Agree","9=Neutral","10=Strongly Agree","11=Agree","12=Strongly Agree","13=Strongly Agree","14=Strongly Agree","15=Yes"],"batch":1532028198071,"_id":"BN2PZWOEZy4O5yhX"}
]'
experiment = fromJSON(input, simplifyDataFrame = TRUE)
experiment <- as.data.frame(experiment)

## Extracting and spreading survey data for proper analysis with likert package:  

for (i in 1:length(experiment$midSurvey)) { 
  experiment$Q1[i] <- substr(experiment$midSurvey[[i]][1], 3, 20)
  experiment$Q2[i] <- substr(experiment$midSurvey[[i]][2], 3, 20)
  experiment$Q3[i] <- substr(experiment$midSurvey[[i]][3], 3, 20)
  experiment$Q4[i] <- substr(experiment$midSurvey[[i]][4], 3, 20)
  experiment$Q5[i] <- substr(experiment$midSurvey[[i]][5], 3, 20)
  experiment$Q6[i] <- substr(experiment$midSurvey[[i]][6], 3, 20)
  experiment$Q7[i] <- substr(experiment$midSurvey[[i]][7], 3, 20)
  experiment$Q8[i] <- substr(experiment$midSurvey[[i]][8], 3, 20)
  experiment$Q9[i] <- substr(experiment$midSurvey[[i]][9], 3, 20)
  experiment$Q10[i] <- substr(experiment$midSurvey[[i]][10], 4, 20)
  experiment$Q11[i] <- substr(experiment$midSurvey[[i]][11], 4, 20)
  experiment$Q12[i] <- substr(experiment$midSurvey[[i]][12], 4, 20)
  experiment$Q13[i] <- substr(experiment$midSurvey[[i]][13], 4, 20)
  experiment$Q14[i] <- substr(experiment$midSurvey[[i]][14], 4, 20)
  experiment$RTQ[i] <- substr(experiment$midSurvey[[i]][15], 4, 20)
} 

```

## Setting up data for likert analysis: 

```{r}
## Convert to factor for analysis and set up levels: 
levels <- c("Strongly Disagree", "Disagree", "Neutral","Agree", "Strongly Agree") 
levels2 <- c("Yes","No")
experiment2 <- experiment %>% 
  mutate_at(.vars = vars(Q1:Q14), funs(factor(., levels = levels))) %>% 
  mutate_at(.vars = vars(RTQ), funs(factor(., levels = levels2))) 

## Summary statistics / basic descriptive stats for survey factors: 
summary(experiment2[,8:22])
## Example of table distribution for a specific survey question and repeat team question: 
table(experiment2[, 8], as.numeric(experiment2[, 8]))
## repeat team question: 
table(experiment2[, 22], as.numeric(experiment2[, 22]))
```

## Setting up likert data and plotting results: 

```{r}
experiment3 <- experiment2
names(experiment3) <- c("userID", "room", "name", "round", "midSurvey", "batch", "_id", "1. The members of this team could work for a long time together" 
                       , "2. Most of the members of this team would welcome the opportunity to work as a group again in the future." , 
                       "3. This team has the capacity for long-term success.", 
                       "4. This team has what it takes to be effective in the future.", 
                       "5. This team would work well together in the future." , 
                       " 6. This team has positioned itself well for continued success.",  
                       " 7. This team has the ability to perform well in the future. ", 
                       " 8. This team has the ability to function as an ongoing unit." , 
                       " 9. This team should continue to function as a unit. ", 
                       " 10. This team has the resources to perform well in the future. ", 
                       " 11. This team is well positioned for growth over time. ", 
                       " 12. This team can develop to meet future challenges. ", 
                       " 13. This team has the capacity to sustain itself. ", 
                       " 14. This team has what it takes to endure in future performance episodes." , 
                       "Would you like to continue to work with this team?") 

## Basic likert plots: 
likert.out <- likert(experiment3[,8:21])
xtable(likert.out)
plot(likert.out)
## Likert heat plot:
plot(likert.out,	type='heat',	wrap=30,	text.size=4)
## Likert density curve: 
plot(likert.out,	type='density')
## Likert density plot with a subset of questions: 
likert.out2 <- likert(experiment3[,8:12])

likert.out.group <- likert(experiment2, grouping = experiment2$RTQ) 
plot(likert.out.group, group.order=experiment2$RTQ) + theme(text=element_text(size=14)) 
```

## Converting to numeric to set up for testing: 

```{r}
for (i in 1:nrow(experiment2)) {
experiment2$nominalQ1 <- as.numeric(experiment2$Q1) 
experiment2$nominalQ2 <- as.numeric(experiment2$Q2)
experiment2$nominalQ3 <- as.numeric(experiment2$Q3)
experiment2$nominalQ4 <- as.numeric(experiment2$Q4)
experiment2$nominalQ5 <- as.numeric(experiment2$Q5)
experiment2$nominalQ6 <- as.numeric(experiment2$Q6)
experiment2$nominalQ7 <- as.numeric(experiment2$Q7)
experiment2$nominalQ8 <- as.numeric(experiment2$Q8)
experiment2$nominalQ9 <- as.numeric(experiment2$Q9)
experiment2$nominalQ10 <- as.numeric(experiment2$Q10)
experiment2$nominalQ11 <- as.numeric(experiment2$Q11)
experiment2$nominalQ12 <- as.numeric(experiment2$Q12)
experiment2$nominalQ13 <- as.numeric(experiment2$Q13)
experiment2$nominalQ14 <- as.numeric(experiment2$Q14)
experiment2$nominalQ15 <- as.numeric(experiment2$RTQ)                                 
} 

## Loop to create sums for data analysis: 

for (i in 1:nrow(experiment2)) {
  experiment2$sum[i] <- sum(experiment2[,23:36][i])                             
} 

## Assessing parametric normality: 

hist(experiment2$sum,xlab="Sum of scores",main="")
```

## Importing and cleaning blacklist json: 
```{r}
input2 = '[{"userID":"qL44K5kHvqZwHuzcAAAA","name":"niceWolf","midSurvey":"blacklist-q1=1","batch":1532028198071,"_id":"BGXqnkNyGqqpHP2c"}
,{"userID":"1pob6EHCHwzLa7nUAAAB","name":"culturedCat","midSurvey":"blacklist-q1=2","batch":1532028198071,"_id":"ELtcSJgRysGlBB77"}
,{"userID":"FGzkFbpp8ejWVSm0AAAC","name":"niceRabbit","midSurvey":"blacklist-q1=2","batch":1532028198071,"_id":"V37TWQFeRT0CwIf6"}
,{"userID":"-2emRdtAMYWaULymAAAD","name":"conventionalDeer","midSurvey":"blacklist-q1=2","batch":1532028198071,"_id":"uXGDrroAYDrKaIVh"}]'

blacklist = fromJSON(input2, simplifyDataFrame = TRUE)
blacklist <- as.data.frame(blacklist)

for (i in 1:length(blacklist$midSurvey)) { 
  blacklist$blacklist[i] <- substr(blacklist$midSurvey[i], 14, 20)
} 

levels3 <- c(1:3)
blacklist <- blacklist %>% 
  mutate_at(.vars = vars(blacklist), funs(factor(., levels = levels3))) 

## Merging for stats tests: 

merged <- merge(experiment2,blacklist,by="name") 
```

Due to the ordinal nature of the data we cannot use parametric techniques to analyse Likert type data; Analysis
of variance techniques include;
• Mann Whitney test.
• Kruskal Wallis test.

Regression techniques include;
• Ordered logistic regression or;
• Multinomial logistic regression.
• Alternatively collapse the levels of the Dependent variable into two levels and run binary logistic regression.

## Data analysis: 
```{r}
## Use Wilcox test to test for a difference in scoring tendancies between exactly 2 groups: 

wilcox.test(sum~room,data=merged)
wilcox.test(sum~nominalQ15,data=merged)

## Use Kruskal-Wallis Test to test for a difference in scoring tendancies between 2+ groups: 

kruskal.test(sum~blacklist,data=merged)
kruskal.test(sum~round,data=merged)

## To test for a difference in means between two groups: use Anova to treat this type of data if we there is a “normally” distributed continious
## independent variable is to flip the variables around. 

anova(lm(blacklist~sum,data=merged))

## Chi-Square test. The Chi-Square test can be used if we combine the data into nominal categories, this
## compares the observed numbers in each category with those expected (i.e. equal proportions), we asses if any
## observed discrepancies (from our theory of equal proportions) can be reasonably put down to chance.
## The numbers in each nominal category are shown below;

```

Parametric analysis of ordinary averages of Likert scale data is justifiable by the Central Limit
Theorem, analysis of variance techniques incude;
• t-test.
• ANOVA.
• Linear regression procedures

```{r}
## Parametric inference: 
hist(merged$sum,xlab="Sum of scores",main="")

## Depending on the results from this test, we can determine if we are justified in using a parametric test. 

## If, data passes normality tests, then we can use the following tests: 

## T-Test. We can use a two-sample T-test to asses if there is a difference in the scores of specific groups: 

## Examples: 

## First use a boxplot for visualization to identify a relationship. 

boxplot(sum~nominalQ15,data=merged,names=c("Continue working with team",
                                                "Stop working with team"))
boxplot(sum~blacklist,data=merged,names=c("Blacklist Team 1",
                                                "Blacklist Team 2", "Indifferent"))

t.test(sum~nominalQ15, data=merged)
t.test(sum~blacklist, data=merged)
```

```{r}
## Example of many ways to group (e.g. group by round & select specific columns of interest like Q1-Q14 & RTQ only):  

experiment %>%
  group_by(round) %>%
  select(room, round, name, Q1:Q14, RTQ)

## Example of selecting a specific round & room: 

test <- experiment %>%
  filter(round == "1", room == "A") %>% 
  select(room, round, Q1:Q14)

## Example of a few extremely basic exploratory plots: 

## Distribution of scale question answers for Q1 in room A across all rounds: 

g <- ggplot(experiment, aes(x=Q1, fill=factor(room)))
g + geom_bar(position="dodge") +
  xlab(label="Likert Responses") +
  labs(title="Distribution of Q1 Responses by Room", fill="Room")

## Distribution of repeat team question answers for all rooms: 

g <- ggplot(experiment, aes(x=RTQ, fill=factor(room)))
g + geom_bar(position="dodge") +
  xlab(label="Likert responses") +
  labs(title="Distribution of repeat team questions across all teams", fill="Room")

## Distribution of repeat team questions answers by round: 

g <- ggplot(experiment, aes(x=RTQ, fill=factor(round)))
g + geom_bar(position="dodge") +
  xlab(label="Likert responses") +
  labs(title="Distribution of repeat team across all teams", fill="Round")

## Distribution of repeat team questions answers by room: 

g2 <- ggplot(experiment, aes(x=round, fill=factor(room)))
g2 + geom_bar() + facet_grid(.~RTQ) + 
  theme(
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank()) + 
  xlab(label="") + 
  ylab(label="") + 
  labs(title="Distribution of answers for repeat team question", fill="Room")

## Example pie chart: 

pie <- ggplot(experiment, aes(x = "", fill = factor(RTQ))) + 
  geom_bar(width = 1) +
  theme(axis.line = element_blank(), 
        plot.title = element_text(hjust=0.5)) + 
  labs(fill="Repeat team question answers:", 
       x=NULL, 
       y=NULL, 
       title="Repeat team question across all teams and rounds", 
       caption="")
pie + coord_polar(theta = "y", start=0)

## More cleaning // dplyr examples: 

experimentround2 <- experiment %>% 
  filter(round=="2")
```

## Initial code for exploratory factor analysis: 
```{r}
out <- factanal(covmat=cor(experimentfactor, use="complete.obs"), factors=5, rotation="varimax")
corMatrix <- cor(experimentfactor, use="complete.obs" )
experimentfactor <- experiment2[,23:36]

## Finding out the right number of factor to select for factor analysis. 
## We will do this via parallel analysis 
parallel<-fa.parallel(experimentfactor, fm='minres', fa='fa')

# Now that we’ve arrived at probable number number of factors, let’s start off with 3 as the number of factors.
# We will select oblique rotation (rotate = “oblimin”) as we believe that there is correlation
# in the factors. Note that Varimax rotation is used under the assumption that the factors are 
# completely uncorrelated. We will use Ordinary Least Squared/Minres factoring (fm = “minres”), 
# as it is known to provide results similar to Maximum Likelihood without assuming multivariate normal distribution and derives solutions through 
# iterative eigendecomposition like principal axis.

threefactor <- fa(experimentfactor,nfactors = 3,rotate = "oblimin",fm="minres")
print(threefactor)

# The blue line shows eigenvalues of actual data and the two red lines (placed on top of each other) 
# show simulated and resampled data. Here we look at the large drops in the actual data and spot the 
# point where it levels off to the right. Also we locate the point of inflection -
# the point where the gap between simulated data and actual data tends to be minimum.
# Looking at this plot and parallel analysis, anywhere between 2 to 5 factors factors would
# be good choice.

print(threefactor$loadings,cutoff = 0.3)
fourfactor <- fa(experimentfactor,nfactors = 4,rotate = "oblimin",fm="minres")
fa.diagram(fourfactor)

## Adequacy test: 
# The root mean square of residuals (RMSR) is 0.05. This is acceptable as this value should 
# be closer to 0. Next we should check RMSEA (root mean square error of approximation) index. 
# Its value, X, good fit if below 0.05. 
# Finally, the Tucker-Lewis Index (TLI) is X - needs to be over 0.9.


experimentfactor <- experiment2[,23:36]
corMatrix <- cor(experimentfactor, use="complete.obs" )
out <- fa(r=corMatrix, factors=5 )
print(out$loadings, cutoff=0.3)
fa.diagram(out)
```

## Initial code for confirmatory factor analysis: 
```{r}
describe(experimentfactor)
# In other words, “are there meaningful latent factors to be found within the data?” We can check two things: (1)
# Bartlett’s test of sphericity; and (2) the Kaiser-Meyer-Olkin measure of sampling adequacy.

# Bartlett’s Test of Sphericity
# The most liberal test is Bartlett’s test of sphericity - this evaluates whether or not the variables intercorrelate
# at all, by evaluating the observed correlation matrix against an “identity matrix” (a matrix with ones along
# the principal diagonal, and zeroes everywhere else). If this test is not statistically significant, you should not
# employ a factor analysis.

cortest.bartlett(experimentfactor)

# Tests results tell us that least some of the variables are correlated with each other.

# The Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy is a better measure of factorability. The
# KMO tests to see if the partial correlations within your data are close enough to zero to suggest that there is
# at least one latent factor underlying your variables. 

KMO(experimentfactor)

# Determining the number of factors to extract: 

# Parallel analysis: 

# A parallel analysis involves generating random correlation matrices and after factor analyzing them, comparing
# the resulting eigenvalues to the eigenvalues of the observed data. The idea behind this method is that
# observed eigenvalues that are higher than their corresponding random eigenvalues are more likely to be from
# “meaningful factors” than observed eigenvalues that are below their corresponding random eigenvalue.

https://www.uwo.ca/fhs/tc/labs/10.FactorAnalysis.pdf 

```

