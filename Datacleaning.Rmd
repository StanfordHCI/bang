---
title: "R Survey Analysis"
output: html_document
---

## Setting up your R workspace and downloading necessary packages // libraries 

```{r setup}
## Install packages outside of notebook environment.
install.packages(c("psych" ,"xtable", "GPArotation", "tidyverse", "purrr", "dplyr", "jsonlite", "likert", "lavaan", "dplyr", "stringr", "ggplot2", 
                   "caret", "nFactors", "GPArotation", "Scale"))
## loaad libraries
library(psych)
library(xtable)
library(GPArotation)
library(psych)
library(dplyr)
library(jsonlite)
library(likert) 
library(lavaan) 
library(stringr)
library(ggplot2)
library(caret)
library(nFactors)
library(GPArotation)
library(Scale)
library(ggplot2)
library(ggthemes)
theme_set(theme_classic())
```

## Inputting data: 
```{r}
## set-up your directory and perform initial cleaning: 
rm(list=ls())
setwd("/Users/allieblaising/desktop/bang/R") 
getwd()
dataPath = "../.data"

extractSurvey = function(frame,survey) {
  rounds = seq(1,length(frame$results.format[[1]]))
  roundResponses = lapply(rounds, function(round) {
    getCol = paste("results.",survey,".",round, sep="")
    surveyCols = Filter(function(x) grepl(getCol,x),names(frame))
    newCols = lapply(surveyCols, function(x) gsub(getCol,paste("results.",survey, sep=""),x) )
    surveyFrame = frame[,surveyCols]
    if (is.null(newCols)) {return("No newCols")}
    names(surveyFrame) = newCols
    surveyFrame$id = frame$id
    surveyFrame$round = round
    surveyFrame$blacklist = frame$blacklistCheck
    return(surveyFrame)
  })
  return(Reduce(rbind,roundResponses))
}

survey = 'viabilityCheck'
batches = dir(dataPath, pattern = "^[0-9]+$" )
completeBatches = Filter(function(batch) { 
  if (any(dir(paste(dataPath,batch,sep="/")) == "batch.json") && (any(dir(paste(dataPath,batch,sep="/")) == "users.json")) ) {
    batchData = read_json(paste(dataPath,batch,"batch.json",sep="/"), simplifyVector = TRUE)
    return(any(batchData$batchComplete == TRUE))
  } 
  return(FALSE)
}, batches)

userFiles = lapply(completeBatches, function(batch){
  userFile = read_json(paste(dataPath,batch,"users.json",sep="/"), simplifyVector = TRUE)
  return(flatten(userFile, recursive = TRUE))
})

overlappingFiles = Reduce(function(x,y) {
  merge(x, y, all=TRUE)
}, userFiles)

myData = extractSurvey(overlappingFiles,survey)
surveyCols = setdiff(names(myData), c("results.viabilityCheck.15", "id", "round"))

## extracting relevant surveys and merging: 
myData <- na.omit(myData)
myData$blacklist <- unlist(overlappingFiles$results.blacklistCheck)
myData$batch <- overlappingFiles$batch
survey2 = 'psychologicalSafety'
survey3 = 'blacklistCheck'
psychSafety <- extractSurvey(overlappingFiles,survey2)
data <- merge(psychSafety, myData, by=c("id", "round"))
data <- rename(data, "repeatTeam" = results.viabilityCheck.15)

## Preparing factors: 
levels <- c("Strongly Disagree", "Disagree", "Neutral","Agree", "Strongly Agree") 
levels2 <- c("1","2","3")
levels3 <- c("Yes", "No")
clean <- data %>% 
  mutate_at(.vars = vars(contains("results.viabilityCheck")), funs(factor(., levels = levels))) %>%
  mutate_at(.vars = vars(contains("results.psychologicalSafety")), funs(factor(., levels = levels))) %>%
  mutate_at(.vars = vars(contains("results.blacklistCheck")), funs(factor(., levels = levels2))) %>% 
  mutate_at(.vars = vars(contains("repeatTeam")), funs(factor(., levels=levels3))) 
 

## viabilitylikert visuals: 
viabilitylikert <- select(clean, contains("results.viabilityCheck"))
viabilitylabels = c("1. The members of this team could work for a long time together" 
                    , "2. Most of the members of this team would welcome the opportunity to work as a group again in the future." , 
                    "3. This team has the capacity for long-term success.", 
                    "4. This team has what it takes to be effective in the future.", 
                    "5. This team would work well together in the future." , 
                    " 6. This team has positioned itself well for continued success.",  
                    " 7. This team has the ability to perform well in the future. ", 
                    " 8. This team has the ability to function as an ongoing unit." , 
                    " 9. This team should continue to function as a unit. ", 
                    " 10. This team has the resources to perform well in the future. ", 
                    " 11. This team is well positioned for growth over time. ", 
                    " 12. This team can develop to meet future challenges. ", 
                    " 13. This team has the capacity to sustain itself. ", 
                    " 14. This team has what it takes to endure in future performance episodes.") 
names(viabilitylikert) <- rep(viabilitylabels) 
## Likert graph for all of viability: 
likert.out <- likert(viabilitylikert)
plot(likert.out)

## Example of how to create likert graph per round (e.g. round #1): 
viabilitylikert2 <- stats %>% filter(round=="2")
names(viabilitylikert2) <- names(viabilitylabels)
likert.out2 <- likert(viabilitylikert2)
xtable(likert.out)
plot(likert.out)
## more likert examples: 
## Likert heat plot:
plot(likert.out,	type='heat',	wrap=30,	text.size=4)
## Likert density curve: 
plot(likert.out,	type='density')

## Likert for psych safety: 
psychsafetylikert <- select(clean, contains("results.psychologicalSafety"))
psychsafetylabels <- c("1. If you make a mistake on this team, it is often held against you.", 
                      "2. Members of this team are able to bring up problems and tough issues.", 
                      "3. People on this team sometimes reject others for being different.", 
                      "4. It is safe to take a risk on this team.", 
                      "5. It is difficult to ask other members of this team for help.", 
                      "6. No one on this team would deliberately act in a way that undermines my efforts. ", 
                      "7. Working with members of this team, my unique skills and talents are valued and utilized.") 
names(psychsafetylikert) <- rep(psychsafetylabels)
## Likert graph for all of psychsafety: 
likert.out <- likert(psychsafetylikert)
plot(likert.out)

## creating new data frame to numeric for stats tests: 
stats <- clean %>% mutate_if(is.factor, as.numeric)

## creating sums for viability scale visualizations (need to recode psych safety variables before 
## I create visualizations for this: 

## viabilityKeys <- c("results.viabiltiyCheck", "repeatTeam")
## viabilitySum <- stats %>% contains(viabilityKeys) 

for (i in 1:nrow(stats)) {
  stats$sum[i] <- sum(stats[i,10:23])                          
} 
stats$mean <- mean(stats$sum) 

## Interested in just one round or batch #? 
## use this code to filter and plot that subset. 
statsRound2 <- stats %>% filter(round=="2")
statsRoom3 <- stats %>% filter(room=="3")
statsRound2Room3 <- stats %>% filter(room=="3" & round=="2")

# Plot examples (will add more later): 
g <- ggplot(stats, aes(factor(repeatTeam, labels = c("Yes", "No")), sum))
g + geom_boxplot(varwidth=T, fill="plum") + 
  labs(subtitle="Sum of viability measures grouped by repeat team question; N = 2 complete", 
       x="If you had the choice, would you like to work with the same team in a future round? ",
       y="Numeric sum of viability measures questions (range: 14-98)")

g <- ggplot(stats, aes(factor(blacklist)))
g + geom_bar(aes(fill=factor(repeatTeam,  labels = c("Yes", "No"))), width = 0.5) + 
  theme(axis.text.x = element_text(angle=65, vjust=0.6)) +
  labs(title="Frequency of blacklist overlaid with repeat team question", 
       fill="Would you like to \n work with the same team in a \n future round?", x="Frequency of blacklist selection") 

barfill <- "#4271AE"
barlines <- "#1F3552"
meanViabilitySum <- ggplot(stats, aes(x = sum)) +
  geom_histogram(aes(y = ..count..), binwidth = 5,
                 colour = barlines, fill = barfill) +
  scale_x_continuous(name = "Mean viability sum \n across all teams",
                     breaks = seq(0, 98, 14),
                     limits=c(14, 98)) +
  scale_y_continuous(name = "count") +
  ggtitle("Frequency of sum of viability scores: N = 2 complete batches") +
  theme_bw() +
  theme(axis.line = element_line(size=1, colour = "black"),
        panel.grid.major = element_line(colour = "#d3d3d3"),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(), panel.background = element_blank(),
        plot.title = element_text(size = 14, family = "Tahoma", face = "bold"),
        text=element_text(family="Tahoma"),
        axis.text.x=element_text(colour="black", size = 9),
        axis.text.y=element_text(colour="black", size = 9)) +
  geom_vline(xintercept = 57.65, size = 1, colour = "#FF3721",
             linetype = "dashed")
meanViabilitySum

g <- ggplot(stats, aes(x=blacklist, fill=blacklist))
g + geom_bar() + facet_grid(.~repeatTeam) + 
  theme(
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank()) + 
  xlab(label="Blacklist options") + 
  ylab(label="Frequency of blacklist selection") + 
  labs(title="Frequency of blacklist selection broken down by repeat team question ", fill = "Blacklist")

## this plot won't run until we have rooms stored, but template is ready 

g <- ggplot(data, aes(x=main_category, fill=main_category))
g + geom_bar() + facet_grid(.~launched) + 
  theme(
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank()) + 
  xlab(label="Blacklist options") + 
  ylab(label="Frequency of blacklist selection") + 
  labs(title="Frequency of blacklist selection broken down by room #", fill = "Blacklist")
```

## Data analysis: 
```{r}
## NEEDS TO BE UPDATED WITH NEW DATA FORMAT (WILL DO TONIGHT)

## Use Wilcox test to test for a difference in scoring tendancies between exactly 2 groups: 
wilcox.test(sum~room,data=merged)
wilcox.test(sum~nominalQ15,data=merged)

## Use Kruskal-Wallis Test to test for a difference in scoring tendancies between 2+ groups: 

kruskal.test(sum~blacklist,data=merged)
kruskal.test(sum~round,data=merged)

## To test for a difference in means between two groups: use Anova to treat this type of data if we there is a “normally” distributed continious
## independent variable is to flip the variables around. 

anova(lm(blacklist~sum,data=merged))

## Chi-Square test. The Chi-Square test can be used if we combine the data into nominal categories, this
## compares the observed numbers in each category with those expected (i.e. equal proportions), we asses if any
## observed discrepancies (from our theory of equal proportions) can be reasonably put down to chance.
## The numbers in each nominal category are shown below;

```

```{bash}
Due to the ordinal nature of the data we cannot use parametric techniques to analyse Likert type data; Analysis
of variance techniques include;
• Mann Whitney test.
• Kruskal Wallis test.

Regression techniques include;
• Ordered logistic regression or;
• Multinomial logistic regression.
• Alternatively collapse the levels of the Dependent variable into two levels and run binary logistic regression.
```

```{r}
```

Parametric analysis of ordinary averages of Likert scale data is justifiable by the Central Limit
Theorem, analysis of variance techniques incude;
• t-test.
• ANOVA.
• Linear regression procedures

```{r}
## Parametric inference: 
hist(merged$sum,xlab="Sum of scores",main="")

## Depending on the results from this test, we can determine if we are justified in using a parametric test. 

## If, data passes normality tests, then we can use the following tests: 

## T-Test. We can use a two-sample T-test to asses if there is a difference in the scores of specific groups:

## Examples: 

## First use a boxplot for visualization to identify a relationship. 

boxplot(sum~nominalQ15,data=merged,names=c("Continue working with team",
                                                "Stop working with team"))

boxplot(sum~blacklist,data=merged,names=c("Blacklist Team 1",
                                                "Blacklist Team 2", "Indifferent"))

t.test(sum~nominalQ15, data=merged)
t.test(sum~blacklist, data=merged)
```

```{r}
## Example of many ways to group (e.g. group by round & select specific columns of interest like Q1-Q14 & RTQ only):  

experiment %>%
  group_by(round) %>%
  select(room, round, name, Q1:Q14, RTQ)

## Example of selecting a specific round & room: 

test <- experiment %>%
  filter(round == "1", room == "A") %>% 
  select(room, round, Q1:Q14)

## Example of a few extremely basic exploratory plots: 

## Distribution of scale question answers for Q1 in room A across all rounds: 

g <- ggplot(experiment, aes(x=Q1, fill=factor(room)))
g + geom_bar(position="dodge") +
  xlab(label="Likert Responses") +
  labs(title="Distribution of Q1 Responses by Room", fill="Room")

## Distribution of repeat team question answers for all rooms: 

g <- ggplot(experiment, aes(x=RTQ, fill=factor(room)))
g + geom_bar(position="dodge") +
  xlab(label="Likert responses") +
  labs(title="Distribution of repeat team questions across all teams", fill="Room")

## Distribution of repeat team questions answers by round: 

g <- ggplot(experiment, aes(x=RTQ, fill=factor(round)))
g + geom_bar(position="dodge") +
  xlab(label="Likert responses") +
  labs(title="Distribution of repeat team across all teams", fill="Round")

## Distribution of repeat team questions answers by room: 

g2 <- ggplot(experiment, aes(x=round, fill=factor(room)))
g2 + geom_bar() + facet_grid(.~RTQ) + 
  theme(
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank()) + 
  xlab(label="") + 
  ylab(label="") + 
  labs(title="Distribution of answers for repeat team question", fill="Room") 


## Example pie chart: 

pie <- ggplot(experiment, aes(x = "", fill = factor(RTQ))) + 
  geom_bar(width = 1) +
  theme(axis.line = element_blank(), 
        plot.title = element_text(hjust=0.5)) + 
  labs(fill="Repeat team question answers:", 
       x=NULL, 
       y=NULL, 
       title="Repeat team question across all teams and rounds", 
       caption="")
pie + coord_polar(theta = "y", start=0)

## More cleaning // dplyr examples: 

experimentround2 <- experiment %>% 
  filter(round=="2")

## Graphs: 

Q4 <- survey[,c(6,20:25)]

# aggregate by Text, computing means = percent respondents who checked box
Q4 <- aggregate( . ~ Text, data=Q4, mean)

# make table long form for ggplot
Q4 <- melt(Q4,id.var="Text")

c("")

ggplot() + 
  geom_bar(
    aes(x=variable,fill=Text,y=value),
    data=Q4,
    stat="identity", position="dodge") + 
  coord_flip() +
  ggtitle("04. When using the text in electronic form, do you....") +
  theme(legend.position = "bottom",
        axis.title.x = element_blank()) +
  guides(fill=guide_legend(title=NULL,ncol=1)) +
  scale_fill_brewer(palette="Dark2") +
  scale_y_continuous(labels = scales::percent)

```

## Initial code for exploratory factor analysis: 
```{r}
out <- factanal(covmat=cor(experimentfactor, use="complete.obs"), factors=5, rotation="varimax")
corMatrix <- cor(experimentfactor, use="complete.obs" )
experimentfactor <- experiment2[,23:36]

## Finding out the right number of factor to select for factor analysis. 
## We will do this via parallel analysis 
parallel<-fa.parallel(experimentfactor, fm='minres', fa='fa')

# Now that we’ve arrived at probable number number of factors, let’s start off with 3 as the number of factors.
# We will select oblique rotation (rotate = “oblimin”) as we believe that there is correlation
# in the factors. Note that Varimax rotation is used under the assumption that the factors are 
# completely uncorrelated. We will use Ordinary Least Squared/Minres factoring (fm = “minres”), 
# as it is known to provide results similar to Maximum Likelihood without assuming multivariate normal distribution and derives solutions through 
# iterative eigendecomposition like principal axis.

threefactor <- fa(experimentfactor,nfactors = 3,rotate = "oblimin",fm="minres")
print(threefactor)

# The blue line shows eigenvalues of actual data and the two red lines (placed on top of each other) 
# show simulated and resampled data. Here we look at the large drops in the actual data and spot the 
# point where it levels off to the right. Also we locate the point of inflection -
# the point where the gap between simulated data and actual data tends to be minimum.
# Looking at this plot and parallel analysis, anywhere between 2 to 5 factors factors would
# be good choice.

print(threefactor$loadings,cutoff = 0.3)
fourfactor <- fa(experimentfactor,nfactors = 4,rotate = "oblimin",fm="minres")
fa.diagram(fourfactor)

## Adequacy test: 
# The root mean square of residuals (RMSR) is 0.05. This is acceptable as this value should 
# be closer to 0. Next we should check RMSEA (root mean square error of approximation) index. 
# Its value, X, good fit if below 0.05. 
# Finally, the Tucker-Lewis Index (TLI) is X - needs to be over 0.9.


## strategy once blacklist is in 

experimentfactor <- experiment2[,23:36]
corMatrix <- cor(experimentfactor, use="complete.obs" )
out <- fa(r=corMatrix, factors=5 )
print(out$loadings, cutoff=0.3)
fa.diagram(out)
```

## Initial code for confirmatory factor analysis: 
```{r}
describe(experimentfactor)
# In other words, “are there meaningful latent factors to be found within the data?” We can check two things: (1)
# Bartlett’s test of sphericity; and (2) the Kaiser-Meyer-Olkin measure of sampling adequacy.

# Bartlett’s Test of Sphericity
# The most liberal test is Bartlett’s test of sphericity - this evaluates whether or not the variables intercorrelate
# at all, by evaluating the observed correlation matrix against an “identity matrix” (a matrix with ones along
# the principal diagonal, and zeroes everywhere else). If this test is not statistically significant, you should not
# employ a factor analysis.

cortest.bartlett(experimentfactor)

# Tests results tell us that least some of the variables are correlated with each other.

# The Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy is a better measure of factorability. The
# KMO tests to see if the partial correlations within your data are close enough to zero to suggest that there is
# at least one latent factor underlying your variables. 

KMO(experimentfactor)

# Determining the number of factors to extract: 

# Parallel analysis: 

# A parallel analysis involves generating random correlation matrices and after factor analyzing them, comparing
# the resulting eigenvalues to the eigenvalues of the observed data. The idea behind this method is that
# observed eigenvalues that are higher than their corresponding random eigenvalues are more likely to be from
# “meaningful factors” than observed eigenvalues that are below their corresponding random eigenvalue.

https://www.uwo.ca/fhs/tc/labs/10.FactorAnalysis.pdf 

```

